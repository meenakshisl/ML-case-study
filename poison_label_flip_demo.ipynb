{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Label-Flip Poisoning on ML-based Malware Detectors\n",
        "\n",
        "Project notebook for an 8–10 minute demo of label-flip poisoning against classic ML malware detectors.\n",
        "\n",
        "- Paper: Aryal et al., \"Analysis of Label-Flip Poisoning Attack on Machine Learning Based Malware Detector\" — http://arxiv.org/pdf/2301.01044\n",
        "- Dataset: Kaggle competition \"Malware detection\" — https://www.kaggle.com/competitions/malware-detection/data\n",
        "\n",
        "This notebook trains 8 models and compares baseline vs. 10% and 20% label-flip poisoning on the training set while keeping the test set clean.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paper context and goals\n",
        "- Paper: Aryal et al., \"Analysis of Label-Flip Poisoning Attack on Machine Learning Based Malware Detector\" (`http://arxiv.org/pdf/2301.01044`)\n",
        "- Threat model: training-time data poisoning via random label flipping; test set remains clean.\n",
        "- Goal: quantify robustness of 8 classic ML models under 10% and 20% label-flip poisoning.\n",
        "- Dataset note: The paper used VirusTotal/VirusShare; here we use your local tabular dataset in the same spirit (binary malware label).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Environment setup\n",
        "Install Python dependencies required for data processing, modeling, and plotting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "pip -q install --upgrade pip && \\\n",
        "  pip -q install numpy pandas scikit-learn matplotlib seaborn tabulate kaggle joblib\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Core imports and display settings\n",
        "Load core libraries (NumPy, Pandas, Seaborn/Matplotlib) and set display options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "palette = sns.color_palette(\"deep\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Helpers and utilities\n",
        "Import shared helpers from `utils.py` if available; otherwise fall back to minimal inline implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers: import utils.py if present; otherwise define minimal fallbacks\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    sys.path.append(str(Path.cwd()))\n",
        "    from utils import (\n",
        "        infer_label_column, ensure_binary_labels, split_features_labels,\n",
        "        build_models, build_pipelines, flip_labels, run_experiment,\n",
        "    )\n",
        "    print(\"Using local utils.py\")\n",
        "except Exception as e:\n",
        "    print(\"utils.py not found; using fallback helpers\")\n",
        "\n",
        "    def infer_label_column(columns):\n",
        "        for c in [\"label\",\"Label\",\"target\",\"Target\",\"class\",\"Class\",\"malware\",\"Malware\",\"is_malware\",\"HasDetections\"]:\n",
        "            if c in columns: return c\n",
        "        return None\n",
        "\n",
        "    def ensure_binary_labels(y):\n",
        "        if y.dtype == bool:\n",
        "            return y.astype(int)\n",
        "        vals = sorted(pd.unique(y))\n",
        "        assert len(vals) == 2, f\"Expected binary labels, got {vals}\"\n",
        "        m = {vals[0]: 0, vals[1]: 1}\n",
        "        return y.map(m).astype(int)\n",
        "\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    from sklearn.linear_model import SGDClassifier, LogisticRegression, Perceptron\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    from sklearn.svm import LinearSVC\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "    def split_features_labels(df, label_col, drop_non_numeric=True):\n",
        "        y = ensure_binary_labels(df[label_col])\n",
        "        X = df.drop(columns=[label_col])\n",
        "        num_cols = list(X.select_dtypes(include=[np.number]).columns)\n",
        "        cat_cols = [] if drop_non_numeric else [c for c in X.columns if c not in num_cols]\n",
        "        if drop_non_numeric:\n",
        "            X = X[num_cols]\n",
        "        return X, y, num_cols, cat_cols\n",
        "\n",
        "    def build_models(random_state=42):\n",
        "        return {\n",
        "            \"SGD\": SGDClassifier(random_state=random_state, max_iter=1000, tol=1e-3),\n",
        "            \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=random_state, n_jobs=-1),\n",
        "            \"LogisticRegression\": LogisticRegression(max_iter=1000, solver=\"liblinear\"),\n",
        "            \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "            \"LinearSVM\": LinearSVC(random_state=random_state),\n",
        "            \"DecisionTree\": DecisionTreeClassifier(random_state=random_state),\n",
        "            \"Perceptron\": Perceptron(random_state=random_state, max_iter=1000),\n",
        "            \"MLP\": MLPClassifier(hidden_layer_sizes=(128,), max_iter=100, early_stopping=True, random_state=random_state),\n",
        "        }\n",
        "\n",
        "    def _needs_scaling(name):\n",
        "        return name in {\"SGD\",\"LogisticRegression\",\"KNN\",\"LinearSVM\",\"Perceptron\",\"MLP\"}\n",
        "\n",
        "    def build_pipelines(models, numeric_cols, categorical_cols):\n",
        "        pipes = {}\n",
        "        for name, model in models.items():\n",
        "            num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
        "            if _needs_scaling(name):\n",
        "                num_steps.append((\"scaler\", StandardScaler()))\n",
        "            pre = ColumnTransformer([\n",
        "                (\"num\", Pipeline(num_steps), numeric_cols),\n",
        "                (\"cat\", \"drop\", categorical_cols),\n",
        "            ])\n",
        "            pipes[name] = Pipeline([(\"pre\", pre), (\"clf\", model)])\n",
        "        return pipes\n",
        "\n",
        "    def flip_labels(y, flip_fraction, rng_seed=42):\n",
        "        rng = np.random.default_rng(rng_seed)\n",
        "        y_p = y.copy()\n",
        "        n = len(y_p)\n",
        "        k = int(flip_fraction * n)\n",
        "        if k > 0:\n",
        "            idx = rng.choice(n, size=k, replace=False)\n",
        "            if hasattr(y_p, 'iloc'):\n",
        "                y_p.iloc[idx] = 1 - y_p.iloc[idx]\n",
        "            else:\n",
        "                y_p[idx] = 1 - y_p[idx]\n",
        "        else:\n",
        "            idx = np.array([], dtype=int)\n",
        "        return (y_p.values if hasattr(y_p, 'values') else y_p), idx\n",
        "\n",
        "    def run_experiment(X_train, y_train, X_test, y_test, pipelines, flip_fracs=(0.0,0.1,0.2), seed=42):\n",
        "        rows = []\n",
        "        cms = {}\n",
        "        for frac in flip_fracs:\n",
        "            y_tr_p, _ = flip_labels(pd.Series(y_train), frac, rng_seed=seed)\n",
        "            for name, pipe in pipelines.items():\n",
        "                pipe.fit(X_train, y_tr_p)\n",
        "                y_pred = pipe.predict(X_test)\n",
        "                rows.append({\n",
        "                    \"model\": name,\n",
        "                    \"flip_frac\": frac,\n",
        "                    \"acc\": accuracy_score(y_test, y_pred),\n",
        "                    \"prec\": precision_score(y_test, y_pred, zero_division=0),\n",
        "                    \"rec\": recall_score(y_test, y_pred, zero_division=0),\n",
        "                })\n",
        "                cms[(name, frac)] = confusion_matrix(y_test, y_pred)\n",
        "        return pd.DataFrame(rows), cms\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Optional: Kaggle authentication (skip if using a local CSV)\n",
        "If you prefer downloading via Kaggle in Colab, place `kaggle.json` under `~/.kaggle/`. Otherwise, skip this section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) Kaggle authentication\n",
        "- Only needed if you want to download from Kaggle. For local files, skip this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5) (Optional) Download via Kaggle CLI\n",
        "If you already have the dataset locally, skip this section entirely.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Configure dataset path (local CSV)\n",
        "Point `INPUT_CSV` to your local dataset file. If left blank, the notebook will try to auto-detect a CSV in the current tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6b) Use a local CSV (recommended)\n",
        "Set `INPUT_CSV` to the path of your already-downloaded dataset. If left blank, the notebook will try to auto-detect a CSV in the current tree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ad of the "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "downloaded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7) Load, split, build models, and run experiments\n",
        "- Load the full dataset and infer the label column.\n",
        "- Split into stratified train/test.\n",
        "- Build 8 models and their pipelines.\n",
        "- Run baseline (clean) and poisoned (10%, 20%) training and evaluate on clean test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local CSV path (recommended): set INPUT_CSV to your downloaded dataset file\n",
        "INPUT_CSV = \"\"  # e.g., \"/Users/you/Downloads/malware/train.csv\"\n",
        "\n",
        "from glob import glob\n",
        "if not INPUT_CSV:\n",
        "    candidates = glob(\"**/*.csv\", recursive=True)\n",
        "    print(\"Found CSV candidates:\", candidates[:5])\n",
        "    INPUT_CSV = candidates[0] if candidates else None\n",
        "\n",
        "print(\"Using:\", INPUT_CSV)\n",
        "import os\n",
        "assert INPUT_CSV and os.path.exists(INPUT_CSV), \"Please set INPUT_CSV to your local CSV path.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sanity checks (paper-aligned)\n",
        "Quick checks before modeling:\n",
        "- Dataset shape\n",
        "- Inferred label column and class balance\n",
        "- Top missing-value ratios (helps understand imputation impact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Label column:\", label_col)\n",
        "print(\"Class balance (head):\")\n",
        "print(df[label_col].value_counts(normalize=True).head())\n",
        "\n",
        "# Show top-10 columns by missing ratio\n",
        "na_ratio = df.isna().mean().sort_values(ascending=False)\n",
        "print(\"\\nTop-10 missing-value ratios:\\n\", na_ratio.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression, Perceptron\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Try importing local utils; if not present, define minimal helpers inline\n",
        "try:\n",
        "    sys.path.append(str(Path.cwd()))\n",
        "    from utils import (\n",
        "        infer_label_column, ensure_binary_labels, split_features_labels,\n",
        "        build_models, build_pipelines, flip_labels, run_experiment,\n",
        "    )\n",
        "    USE_LOCAL_UTILS = True\n",
        "except Exception as e:\n",
        "    USE_LOCAL_UTILS = False\n",
        "\n",
        "    def infer_label_column(columns):\n",
        "        for c in [\"label\",\"Label\",\"target\",\"Target\",\"class\",\"Class\",\"malware\",\"Malware\",\"is_malware\",\"HasDetections\"]:\n",
        "            if c in columns: return c\n",
        "        return None\n",
        "\n",
        "    def ensure_binary_labels(y):\n",
        "        if y.dtype == bool:\n",
        "            return y.astype(int)\n",
        "        vals = sorted(pd.unique(y))\n",
        "        assert len(vals) == 2, f\"Expected binary labels, got {vals}\"\n",
        "        m = {vals[0]: 0, vals[1]: 1}\n",
        "        return y.map(m).astype(int)\n",
        "\n",
        "    def split_features_labels(df, label_col, drop_non_numeric=True):\n",
        "        y = ensure_binary_labels(df[label_col])\n",
        "        X = df.drop(columns=[label_col])\n",
        "        num_cols = list(X.select_dtypes(include=[np.number]).columns)\n",
        "        cat_cols = [] if drop_non_numeric else [c for c in X.columns if c not in num_cols]\n",
        "        if drop_non_numeric:\n",
        "            X = X[num_cols]\n",
        "        return X, y, num_cols, cat_cols\n",
        "\n",
        "    def build_models(random_state=42):\n",
        "        return {\n",
        "            \"SGD\": SGDClassifier(random_state=random_state, max_iter=1000, tol=1e-3),\n",
        "            \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=random_state, n_jobs=-1),\n",
        "            \"LogisticRegression\": LogisticRegression(max_iter=1000, solver=\"liblinear\"),\n",
        "            \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "            \"LinearSVM\": LinearSVC(random_state=random_state),\n",
        "            \"DecisionTree\": DecisionTreeClassifier(random_state=random_state),\n",
        "            \"Perceptron\": Perceptron(random_state=random_state, max_iter=1000),\n",
        "            \"MLP\": MLPClassifier(hidden_layer_sizes=(128,), max_iter=100, early_stopping=True, random_state=random_state),\n",
        "        }\n",
        "\n",
        "    def _needs_scaling(name):\n",
        "        return name in {\"SGD\",\"LogisticRegression\",\"KNN\",\"LinearSVM\",\"Perceptron\",\"MLP\"}\n",
        "\n",
        "    def build_pipelines(models, numeric_cols, categorical_cols):\n",
        "        pipes = {}\n",
        "        for name, model in models.items():\n",
        "            num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
        "            if _needs_scaling(name):\n",
        "                num_steps.append((\"scaler\", StandardScaler()))\n",
        "            pre = ColumnTransformer([\n",
        "                (\"num\", Pipeline(num_steps), numeric_cols),\n",
        "                (\"cat\", \"drop\", categorical_cols),\n",
        "            ])\n",
        "            pipes[name] = Pipeline([(\"pre\", pre), (\"clf\", model)])\n",
        "        return pipes\n",
        "\n",
        "    def flip_labels(y, flip_fraction, rng_seed=42):\n",
        "        rng = np.random.default_rng(rng_seed)\n",
        "        y_p = y.copy()\n",
        "        n = len(y_p)\n",
        "        k = int(flip_fraction * n)\n",
        "        if k > 0:\n",
        "            idx = rng.choice(n, size=k, replace=False)\n",
        "            y_p.iloc[idx] = 1 - y_p.iloc[idx]\n",
        "        else:\n",
        "            idx = np.array([], dtype=int)\n",
        "        return y_p.values if hasattr(y_p, 'values') else y_p, idx\n",
        "\n",
        "    def run_experiment(X_train, y_train, X_test, y_test, pipelines, flip_fracs=(0.0,0.1,0.2), seed=42):\n",
        "        rows = []\n",
        "        cms = {}\n",
        "        for frac in flip_fracs:\n",
        "            y_tr_p, _ = flip_labels(pd.Series(y_train), frac, rng_seed=seed)\n",
        "            for name, pipe in pipelines.items():\n",
        "                pipe.fit(X_train, y_tr_p)\n",
        "                y_pred = pipe.predict(X_test)\n",
        "                rows.append({\n",
        "                    \"model\": name,\n",
        "                    \"flip_frac\": frac,\n",
        "                    \"acc\": accuracy_score(y_test, y_pred),\n",
        "                    \"prec\": precision_score(y_test, y_pred, zero_division=0),\n",
        "                    \"rec\": recall_score(y_test, y_pred, zero_division=0),\n",
        "                })\n",
        "                cms[(name, frac)] = confusion_matrix(y_test, y_pred)\n",
        "        return pd.DataFrame(rows), cms\n",
        "\n",
        "# Load dataset\n",
        "assert INPUT_CSV is not None, \"Please set INPUT_CSV to a CSV path.\"\n",
        "if 'df' not in locals():\n",
        "    df = pd.read_csv(INPUT_CSV)\n",
        "label_col = infer_label_column(df.columns)\n",
        "if label_col is None:\n",
        "    raise ValueError(\"Could not infer label column; please set it manually.\")\n",
        "print(\"Label column:\", label_col)\n",
        "\n",
        "# For runtime, optionally sample with stratified approach; default is FULL dataset\n",
        "\n",
        "def stratified_sample(df, label_col, sample_size=10_000, seed=42):\n",
        "    if sample_size <= 0 or sample_size >= len(df):\n",
        "        return df\n",
        "    rng = np.random.default_rng(seed)\n",
        "    parts = []\n",
        "    for v, g in df.groupby(label_col):\n",
        "        frac = len(g) / len(df)\n",
        "        k = max(1, int(round(frac * sample_size)))\n",
        "        idx = rng.choice(len(g), size=min(k, len(g)), replace=False)\n",
        "        parts.append(g.iloc[idx])\n",
        "    return pd.concat(parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "SAMPLE_SIZE = 0  # set >0 (e.g., 10000) to run a faster demo; 0 = full dataset\n",
        "df_sampled = stratified_sample(df, label_col=label_col, sample_size=SAMPLE_SIZE, seed=42)\n",
        "print(\"Rows used:\", len(df_sampled))\n",
        "\n",
        "X, y, num_cols, cat_cols = split_features_labels(df_sampled, label_col=label_col, drop_non_numeric=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y.values, test_size=0.2, random_state=42, stratify=y.values\n",
        ")\n",
        "print(\"Train/Test:\", len(X_train), len(X_test))\n",
        "\n",
        "models = build_models(random_state=42)\n",
        "pipes = build_pipelines(models, numeric_cols=num_cols, categorical_cols=cat_cols)\n",
        "results, cms = run_experiment(\n",
        "    X_train, y_train, X_test, y_test, pipelines=pipes, flip_fracs=(0.0,0.1,0.2), seed=42\n",
        ")\n",
        "results.sort_values([\"model\",\"flip_frac\"], inplace=True)\n",
        "results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pivot_acc = results.pivot(index=\"model\", columns=\"flip_frac\", values=\"acc\")\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.heatmap(pivot_acc, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
        "plt.title(\"Accuracy by Model and Flip Fraction\")\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrices for a couple of models at 20%\n",
        "max_frac = results[\"flip_frac\"].max()\n",
        "for m in [\"LogisticRegression\", \"RandomForest\", \"MLP\"]:\n",
        "    key = (m, max_frac)\n",
        "    if key in cms:\n",
        "        cm = cms[key]\n",
        "        plt.figure(figsize=(3.8,3.2))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False, cmap=\"Blues\",\n",
        "                    xticklabels=[\"Benign\",\"Malware\"], yticklabels=[\"Benign\",\"Malware\"])\n",
        "        plt.title(f\"{m} @ flip={max_frac}\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion\n",
        "- Which models showed the largest degradation as flip increased?\n",
        "- Did precision or recall suffer more? For malware detection, recall is critical (missed malware).\n",
        "- How do results align with the paper's observations?\n",
        "\n",
        "Reference: Aryal et al., http://arxiv.org/pdf/2301.01044\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
